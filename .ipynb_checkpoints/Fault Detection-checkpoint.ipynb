{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55efce20",
   "metadata": {},
   "source": [
    "### Fault Detection using Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b59bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected faults:\n",
      "Index: 5, Value: 50\n",
      "[(5, 50)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_faults(time_series_data, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect faults in time series data using Z-score anomaly detection.\n",
    "    \n",
    "    Args:\n",
    "        time_series_data (list or numpy array): The time series data.\n",
    "        threshold (float): The threshold value to determine anomalies.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of tuples containing the index and value of detected anomalies.\n",
    "    \"\"\"\n",
    "    # Convert data to numpy array\n",
    "    data = np.array(time_series_data)\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    # Calculate Z-scores for each data point\n",
    "    z_scores = (data - mean) / std\n",
    "    \n",
    "    # Find anomalies based on threshold\n",
    "    anomalies = [(i, value) for i, value in enumerate(time_series_data) if abs(z_scores[i]) > threshold]\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Example usage\n",
    "time_series = [1, 2, 3, 4, 5, 50, 6, 7, 8, 9]\n",
    "detected_faults = detect_faults(time_series, threshold=2)\n",
    "\n",
    "if len(detected_faults) > 0:\n",
    "    print(\"Detected faults:\")\n",
    "    for index, value in detected_faults:\n",
    "        print(f\"Index: {index}, Value: {value}\")\n",
    "else:\n",
    "    print(\"No faults detected.\")\n",
    "print(detected_faults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfbd5aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of mean is 9.5\n",
      "The value of std is 13.720422734012244\n",
      "[-0.61951444 -0.54663039 -0.47374634 -0.40086228 -0.32797823  2.95180409\n",
      " -0.25509418 -0.18221013 -0.10932608 -0.03644203]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "time_series_data = [1, 2, 3, 4, 5, 50, 6, 7, 8, 9]\n",
    "threshold = 3\n",
    "# Convert data to numpy array\n",
    "data = np.array(time_series_data)\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "\n",
    "# Calculate Z-scores for each data point\n",
    "z_scores = (data - mean) / std\n",
    "# Find anomalies based on threshold\n",
    "anomalies = [(i, value) for i, value in enumerate(time_series_data) if abs(z_scores[i]) > threshold]\n",
    "print(f'The value of mean is {mean}')\n",
    "print(f'The value of std is {std}')\n",
    "print(z_scores)\n",
    "print(anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0fc14",
   "metadata": {},
   "source": [
    "### Fault detection using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1ba0c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1711\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1533\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1367\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1213\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1069\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0934\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0811\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0697\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0595\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0507\n",
      "Train loss: 0.0434\n",
      "Test loss: 0.1623\n",
      "1/1 [==============================] - 0s 330ms/step\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('shampoo.csv', index_col=0, header=0, parse_dates=True)  # Replace with your dataset\n",
    "values = data['value'].values.reshape(-1, 1)  # Assuming 'value' is the column containing data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(values)\n",
    "\n",
    "# Define sequence length and split into input/output sequences\n",
    "sequence_length = 10\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(scaled_values) - sequence_length):\n",
    "    X.append(scaled_values[i:i+sequence_length])\n",
    "    y.append(scaled_values[i+sequence_length])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Train loss: {train_loss:.4f}')\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data = pd.read_csv('shampoo-test.csv', index_col=0, header=0, parse_dates=True)  # Replace with your new dataset\n",
    "new_values = new_data['value'].values.reshape(-1, 1)\n",
    "new_scaled_values = scaler.transform(new_values)\n",
    "X_new = []\n",
    "for i in range(len(new_scaled_values) - sequence_length):\n",
    "    X_new.append(new_scaled_values[i:i+sequence_length])\n",
    "X_new = np.array(X_new)\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Calculate prediction errors\n",
    "errors = np.abs(predictions - X_new[:, -1])\n",
    "\n",
    "# Define a threshold for fault detection\n",
    "threshold = 0.1  # Adjust based on your requirements\n",
    "\n",
    "# Classify data points as normal or faulty based on the threshold\n",
    "classifications = ['Normal' if error <= threshold else 'Faulty' for error in errors]\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75a237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7ca0158",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Anomaly detection using One-Class SVM\u001b[39;00m\n\u001b[0;32m     14\u001b[0m one_class_svm \u001b[38;5;241m=\u001b[39m OneClassSVM(nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mone_class_svm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_series_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m anomaly_predictions \u001b[38;5;241m=\u001b[39m one_class_svm\u001b[38;5;241m.\u001b[39mpredict(time_series_data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Print the fault and anomaly predictions\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\svm\\_classes.py:1626\u001b[0m, in \u001b[0;36mOneClassSVM.fit\u001b[1;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(params) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1619\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing additional keyword parameters has no effect and is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated in 1.0. An error will be raised from 1.2 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1625\u001b[0m     )\n\u001b[1;32m-> 1626\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intercept_\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\svm\\_base.py:255\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 255\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\svm\\_base.py:315\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    302\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m--> 315\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "File \u001b[1;32msklearn\\svm\\_libsvm.pyx:58\u001b[0m, in \u001b[0;36msklearn.svm._libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not str"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Simulated time series data with faults and anomalies\n",
    "time_series_data = np.array([1.2, 1.3, 1.5, 1.2, 1.4, 100.0, 1.3, 1.2, 1.4, 1.3])\n",
    "\n",
    "# Fault detection using Isolation Forest\n",
    "isolation_forest = IsolationForest(contamination='auto')\n",
    "isolation_forest.fit(time_series_data.reshape(-1, 1))\n",
    "fault_predictions = isolation_forest.predict(time_series_data.reshape(-1, 1))\n",
    "\n",
    "# Anomaly detection using One-Class SVM\n",
    "one_class_svm = OneClassSVM(nu='auto')\n",
    "one_class_svm.fit(time_series_data.reshape(-1, 1))\n",
    "anomaly_predictions = one_class_svm.predict(time_series_data.reshape(-1, 1))\n",
    "\n",
    "# Print the fault and anomaly predictions\n",
    "print(\"Fault Predictions:\")\n",
    "print(fault_predictions)\n",
    "print(\"Anomaly Predictions:\")\n",
    "print(anomaly_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2077809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault Predictions:\n",
      "[ 1  1 -1  1  1 -1  1  1  1  1]\n",
      "Anomaly Predictions:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Simulated time series data with faults and anomalies\n",
    "time_series_data = [1.2, 1.3, 1.5, 1.2, 1.4, 100.0, 1.3, 1.2, 1.4, 1.3]\n",
    "\n",
    "# Convert time series data to a numpy array\n",
    "time_series_data = np.array(time_series_data)\n",
    "\n",
    "# Fault detection using Isolation Forest\n",
    "isolation_forest = IsolationForest(contamination='auto')\n",
    "isolation_forest.fit(time_series_data.reshape(-1, 1))\n",
    "fault_predictions = isolation_forest.predict(time_series_data.reshape(-1, 1))\n",
    "\n",
    "# # Anomaly detection using One-Class SVM\n",
    "# one_class_svm = OneClassSVM(nu='auto')\n",
    "# one_class_svm.fit(time_series_data.reshape(-1, 1))\n",
    "# anomaly_scores = one_class_svm.score_samples(time_series_data.reshape(-1, 1))\n",
    "\n",
    "# # Convert anomaly scores to binary predictions\n",
    "# threshold = 0  # Adjust threshold as needed\n",
    "# anomaly_predictions = np.where(anomaly_scores < threshold, -1, 1)\n",
    "\n",
    "# Print the fault and anomaly predictions\n",
    "print(\"Fault Predictions:\")\n",
    "print(fault_predictions)\n",
    "print(\"Anomaly Predictions:\")\n",
    "# print(anomaly_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86379128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
     ]
    }
   ],
   "source": [
    "data1 = [1,2,3,4,5,6,7,8,9,10]\n",
    "data2 = [2*i for i in data1]\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc867b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2048368229954285\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tools.eval_measures import rmse\n",
    "rmse_val = rmse(data1, data2)\n",
    "print(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40f8fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2048368229954285\n"
     ]
    }
   ],
   "source": [
    "def my_rmse(x, y):\n",
    "    if len(x) != len(y):\n",
    "        return \"Error: The two arguments must have the same length\"\n",
    "    mse = np.square(np.subtract(x, y)).mean()\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "print(my_rmse(data1, data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9be5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1b751b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another test\n"
     ]
    }
   ],
   "source": [
    "print(\"Another test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498c4c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External test\n"
     ]
    }
   ],
   "source": [
    "print(\"External test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce166f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
