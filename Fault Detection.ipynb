{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55efce20",
   "metadata": {},
   "source": [
    "### Fault Detection using Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b59bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected faults:\n",
      "Index: 5, Value: 50\n",
      "[(5, 50)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_faults(time_series_data, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect faults in time series data using Z-score anomaly detection.\n",
    "    \n",
    "    Args:\n",
    "        time_series_data (list or numpy array): The time series data.\n",
    "        threshold (float): The threshold value to determine anomalies.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of tuples containing the index and value of detected anomalies.\n",
    "    \"\"\"\n",
    "    # Convert data to numpy array\n",
    "    data = np.array(time_series_data)\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    # Calculate Z-scores for each data point\n",
    "    z_scores = (data - mean) / std\n",
    "    \n",
    "    # Find anomalies based on threshold\n",
    "    anomalies = [(i, value) for i, value in enumerate(time_series_data) if abs(z_scores[i]) > threshold]\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Example usage\n",
    "time_series = [1, 2, 3, 4, 5, 50, 6, 7, 8, 9]\n",
    "detected_faults = detect_faults(time_series, threshold=2)\n",
    "\n",
    "if len(detected_faults) > 0:\n",
    "    print(\"Detected faults:\")\n",
    "    for index, value in detected_faults:\n",
    "        print(f\"Index: {index}, Value: {value}\")\n",
    "else:\n",
    "    print(\"No faults detected.\")\n",
    "print(detected_faults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfbd5aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of mean is 9.5\n",
      "The value of std is 13.720422734012244\n",
      "[-0.61951444 -0.54663039 -0.47374634 -0.40086228 -0.32797823  2.95180409\n",
      " -0.25509418 -0.18221013 -0.10932608 -0.03644203]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "time_series_data = [1, 2, 3, 4, 5, 50, 6, 7, 8, 9]\n",
    "threshold = 3\n",
    "# Convert data to numpy array\n",
    "data = np.array(time_series_data)\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "\n",
    "# Calculate Z-scores for each data point\n",
    "z_scores = (data - mean) / std\n",
    "# Find anomalies based on threshold\n",
    "anomalies = [(i, value) for i, value in enumerate(time_series_data) if abs(z_scores[i]) > threshold]\n",
    "print(f'The value of mean is {mean}')\n",
    "print(f'The value of std is {std}')\n",
    "print(z_scores)\n",
    "print(anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0fc14",
   "metadata": {},
   "source": [
    "### Fault detection using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1ba0c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2096\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1876\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1671\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1480\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1304\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1140\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0989\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0851\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0726\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0615\n",
      "Train loss: 0.0519\n",
      "Test loss: 0.2071\n",
      "1/1 [==============================] - 0s 322ms/step\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('shampoo.csv', index_col=0, header=0, parse_dates=True)  # Replace with your dataset\n",
    "values = data['value'].values.reshape(-1, 1)  # Assuming 'value' is the column containing data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(values)\n",
    "\n",
    "# Define sequence length and split into input/output sequences\n",
    "sequence_length = 10\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(scaled_values) - sequence_length):\n",
    "    X.append(scaled_values[i:i+sequence_length])\n",
    "    y.append(scaled_values[i+sequence_length])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Train loss: {train_loss:.4f}')\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data = pd.read_csv('shampoo-test.csv', index_col=0, header=0, parse_dates=True)  # Replace with your new dataset\n",
    "new_values = new_data['value'].values.reshape(-1, 1)\n",
    "new_scaled_values = scaler.transform(new_values)\n",
    "X_new = []\n",
    "for i in range(len(new_scaled_values) - sequence_length):\n",
    "    X_new.append(new_scaled_values[i:i+sequence_length])\n",
    "X_new = np.array(X_new)\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Calculate prediction errors\n",
    "errors = np.abs(predictions - X_new[:, -1])\n",
    "\n",
    "# Define a threshold for fault detection\n",
    "threshold = 0.1  # Adjust based on your requirements\n",
    "\n",
    "# Classify data points as normal or faulty based on the threshold\n",
    "classifications = ['Normal' if error <= threshold else 'Faulty' for error in errors]\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75a237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7ca0158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault Predictions:\n",
      "[ 1  1 -1  1  1 -1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Simulated time series data with faults and anomalies\n",
    "time_series_data = np.array([1.2, 1.3, 1.5, 1.2, 1.4, 100.0, 1.3, 1.2, 1.4, 1.3])\n",
    "\n",
    "# Fault detection using Isolation Forest\n",
    "isolation_forest = IsolationForest(contamination='auto')\n",
    "isolation_forest.fit(time_series_data.reshape(-1, 1))\n",
    "fault_predictions = isolation_forest.predict(time_series_data.reshape(-1, 1))\n",
    "\n",
    "# Anomaly detection using One-Class SVM\n",
    "# one_class_svm = OneClassSVM(nu='auto')\n",
    "# one_class_svm.fit(time_series_data.reshape(-1, 1))\n",
    "# anomaly_predictions = one_class_svm.predict(time_series_data.reshape(-1, 1))\n",
    "\n",
    "# Print the fault and anomaly predictions\n",
    "print(\"Fault Predictions:\")\n",
    "print(fault_predictions)\n",
    "# print(\"Anomaly Predictions:\")\n",
    "# print(anomaly_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2077809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault Predictions:\n",
      "[ 1  1 -1  1  1 -1  1  1  1  1]\n",
      "Anomaly Predictions:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Simulated time series data with faults and anomalies\n",
    "time_series_data = [1.2, 1.3, 1.5, 1.2, 1.4, 100.0, 1.3, 1.2, 1.4, 1.3]\n",
    "\n",
    "# Convert time series data to a numpy array\n",
    "time_series_data = np.array(time_series_data)\n",
    "\n",
    "# Fault detection using Isolation Forest\n",
    "isolation_forest = IsolationForest(contamination='auto')\n",
    "isolation_forest.fit(time_series_data.reshape(-1, 1))\n",
    "fault_predictions = isolation_forest.predict(time_series_data.reshape(-1, 1))\n",
    "\n",
    "# # Anomaly detection using One-Class SVM\n",
    "# one_class_svm = OneClassSVM(nu='auto')\n",
    "# one_class_svm.fit(time_series_data.reshape(-1, 1))\n",
    "# anomaly_scores = one_class_svm.score_samples(time_series_data.reshape(-1, 1))\n",
    "\n",
    "# # Convert anomaly scores to binary predictions\n",
    "# threshold = 0  # Adjust threshold as needed\n",
    "# anomaly_predictions = np.where(anomaly_scores < threshold, -1, 1)\n",
    "\n",
    "# Print the fault and anomaly predictions\n",
    "print(\"Fault Predictions:\")\n",
    "print(fault_predictions)\n",
    "print(\"Anomaly Predictions:\")\n",
    "# print(anomaly_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86379128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
     ]
    }
   ],
   "source": [
    "data1 = [1,2,3,4,5,6,7,8,9,10]\n",
    "data2 = [2*i for i in data1]\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc867b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2048368229954285\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tools.eval_measures import rmse\n",
    "rmse_val = rmse(data1, data2)\n",
    "print(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40f8fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2048368229954285\n"
     ]
    }
   ],
   "source": [
    "def root_mse(x, y):\n",
    "    if len(x) != len(y):\n",
    "        return \"Error: The two arguments must have the same length\"\n",
    "    mse = np.square(np.subtract(x, y)).mean()\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "print(root_mse(data1, data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "498c4c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x00\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70803647",
   "metadata": {},
   "source": [
    "### CUSUM ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d02b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae516a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change points detected at indices: [2, 3, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cusum(data, target, threshold):\n",
    "    cumulative_sum = np.zeros_like(data)  # Initialize cumulative sum array\n",
    "    change_points = []  # List to store detected change points\n",
    "    \n",
    "    for i in range(1, len(data)):\n",
    "#         deviation = data[i] - target  # Calculate deviation from the target value\n",
    "        deviation = data[i] - data[i-1]  # Calculate deviation from the target value\n",
    "        cumulative_sum[i] = max(0, cumulative_sum[i-1] + deviation - threshold)  # Update cumulative sum\n",
    "        \n",
    "        if cumulative_sum[i] >= threshold:\n",
    "            change_points.append(i)  # Store the index of detected change point\n",
    "    \n",
    "    return change_points\n",
    "\n",
    "# Example usage\n",
    "data = [10, 9, 11, 12, 8, 9, 7, 13, 14, 9, 8, 7]\n",
    "target = 10  # Reference value\n",
    "threshold = 1  # Threshold for detecting significant deviations\n",
    "\n",
    "change_points = cusum(data, target, threshold)\n",
    "print(\"Change points detected at indices:\", change_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48b1cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "dt = [10, 9, 11, 12, 8, 9, 7, 13, 14, 9, 8, 7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "915cd1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55070f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
