{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deed7b72",
   "metadata": {},
   "source": [
    "### Impor libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2011f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate output stacked lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack, vstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, RepeatVector, TimeDistributed\n",
    "from keras.layers.core import Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4410d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the raw_data before scaling is (43800, 7)\n",
      "(7931, 30, 7)\n",
      "(7931, 40, 7)\n"
     ]
    }
   ],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# Read the data\n",
    "data = read_csv('pollution.csv', header=0, index_col=0)\n",
    "\n",
    "# horizontally stack columns\n",
    "raw_data = data.values\n",
    "print(f'The shape of the raw_data before scaling is {raw_data.shape}')\n",
    "#normalize input features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(raw_data)\n",
    "\n",
    "n_train = 8000\n",
    "#dataset = raw_data[0:n_train,:]\n",
    "dataset = scaled_data[0:n_train,:]\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 30, 40\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc79e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(X, label='train')\n",
    "# plt.plot(y, label='test')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"magnitude\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7926b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1, 6, 51) (100000, 1, 3, 51) (100000, 1, 3, 51)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, None, 51), found shape=(32, 1, 6, 51)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 97>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(X1\u001b[38;5;241m.\u001b[39mshape,X2\u001b[38;5;241m.\u001b[39mshape,y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# evaluate LSTM\u001b[39;00m\n\u001b[0;32m     99\u001b[0m total, correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileyz7ua_0w.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\hba986\\.conda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, None, 51), found shape=(32, 1, 6, 51)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    " \n",
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [randint(1, n_unique-1) for _ in range(length)]\n",
    " \n",
    "# prepare data for the LSTM\n",
    "def get_dataset(n_in, n_out, cardinality, n_samples):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\tfor _ in range(n_samples):\n",
    "\t\t# generate source sequence\n",
    "\t\tsource = generate_sequence(n_in, cardinality)\n",
    "\t\t# define padded target sequence\n",
    "\t\ttarget = source[:n_out]\n",
    "\t\ttarget.reverse()\n",
    "\t\t# create padded input target sequence\n",
    "\t\ttarget_in = [0] + target[:-1]\n",
    "\t\t# encode\n",
    "\t\tsrc_encoded = to_categorical([source], num_classes=cardinality)\n",
    "\t\ttar_encoded = to_categorical([target], num_classes=cardinality)\n",
    "\t\ttar2_encoded = to_categorical([target_in], num_classes=cardinality)\n",
    "\t\t# store\n",
    "\t\tX1.append(src_encoded)\n",
    "\t\tX2.append(tar2_encoded)\n",
    "\t\ty.append(tar_encoded)\n",
    "\treturn array(X1), array(X2), array(y)\n",
    " \n",
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_input, n_output, n_units):\n",
    "\t# define training encoder\n",
    "\tencoder_inputs = Input(shape=(None, n_input))\n",
    "\tencoder = LSTM(n_units, return_state=True)\n",
    "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\tencoder_states = [state_h, state_c]\n",
    "\t# define training decoder\n",
    "\tdecoder_inputs = Input(shape=(None, n_output))\n",
    "\tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\tdecoder_dense = Dense(n_output, activation='softmax')\n",
    "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "\tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\t# define inference encoder\n",
    "\tencoder_model = Model(encoder_inputs, encoder_states)\n",
    "\t# define inference decoder\n",
    "\tdecoder_state_input_h = Input(shape=(n_units,))\n",
    "\tdecoder_state_input_c = Input(shape=(n_units,))\n",
    "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\tdecoder_states = [state_h, state_c]\n",
    "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "\tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\t# return all models\n",
    "\treturn model, encoder_model, decoder_model\n",
    " \n",
    "# generate target given source sequence\n",
    "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "\t# encode\n",
    "\tstate = infenc.predict(source)\n",
    "\t# start of sequence input\n",
    "\ttarget_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "\t# collect predictions\n",
    "\toutput = list()\n",
    "\tfor t in range(n_steps):\n",
    "\t\t# predict next char\n",
    "\t\tyhat, h, c = infdec.predict([target_seq] + state)\n",
    "\t\t# store prediction\n",
    "\t\toutput.append(yhat[0,0,:])\n",
    "\t\t# update state\n",
    "\t\tstate = [h, c]\n",
    "\t\t# update target sequence\n",
    "\t\ttarget_seq = yhat\n",
    "\treturn array(output)\n",
    " \n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [argmax(vector) for vector in encoded_seq]\n",
    " \n",
    "# configure problem\n",
    "n_features = 50 + 1\n",
    "n_steps_in = 6\n",
    "n_steps_out = 3\n",
    "# define model\n",
    "train, infenc, infdec = define_models(n_features, n_features, 128)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# generate training dataset\n",
    "X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 100000)\n",
    "print(X1.shape,X2.shape,y.shape)\n",
    "# train model\n",
    "train.fit([X1, X2], y, epochs=1)\n",
    "# evaluate LSTM\n",
    "total, correct = 100, 0\n",
    "for _ in range(total):\n",
    "\tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "\ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "\tif array_equal(one_hot_decode(y[0]), one_hot_decode(target)):\n",
    "\t\tcorrect += 1\n",
    "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
    "# spot check some examples\n",
    "for _ in range(10):\n",
    "\tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "\ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "\tprint('X=%s y=%s, yhat=%s' % (one_hot_decode(X1[0]), one_hot_decode(y[0]), one_hot_decode(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be533195",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(0, 5, None), slice(None, None, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\pandas\\_libs\\index.pyx:142\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(0, 5, None), slice(None, None, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m dataset \u001b[38;5;241m=\u001b[39m read_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#, header=0, index_col=0)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m n_in, n_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m---> 29\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43msplit_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     31\u001b[0m dataset\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36msplit_sequences\u001b[1;34m(sequences, n_steps_in, n_steps_out)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# gather input and output parts of the pattern\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m seq_x, seq_y \u001b[38;5;241m=\u001b[39m \u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_ix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, sequences[end_ix:out_end_ix, :]\n\u001b[0;32m     22\u001b[0m X\u001b[38;5;241m.\u001b[39mappend(seq_x)\n\u001b[0;32m     23\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(seq_y)\n",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3628\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3623\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m         \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m         \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m         \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3628\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3629\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   3631\u001b[0m \u001b[38;5;66;03m# GH#42269\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DeepLearning\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5637\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   5634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5635\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5636\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(0, 5, None), slice(None, None, None))"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# Split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# Define input sequence\n",
    "dataset = read_csv('test_data.csv')#, header=0, index_col=0)\n",
    "n_in, n_out = 5, 5\n",
    "X, y = split_sequences(dataset, n_in, n_out)\n",
    "print(dataset.shape)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28d0ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "arr = [1,2,3,4,5]\n",
    "for i in range(len(arr)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e5db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
